import { BertTokenizer } from "../../../src/tokenizers.js";
import { BASE_TEST_STRINGS, BERT_TEST_STRINGS } from "../test_strings.js";

export const TOKENIZER_CLASS = BertTokenizer;
export const TEST_CONFIG = {
    "Xenova/bert-base-uncased": {
        SIMPLE: {
            text: BASE_TEST_STRINGS.SIMPLE,
            tokens: ["how", "are", "you", "doing", "?"],
            ids: [101, 2129, 2024, 2017, 2725, 1029, 102],
            decoded: "[CLS] how are you doing? [SEP]",
        },
        SIMPLE_WITH_PUNCTUATION: {
            text: BASE_TEST_STRINGS.SIMPLE_WITH_PUNCTUATION,
            tokens: ["you", "should", "'", "ve", "done", "this"],
            ids: [101, 2017, 2323, 1005, 2310, 2589, 2023, 102],
            decoded: "[CLS] you should've done this [SEP]",
        },
        TEXT_WITH_NUMBERS: {
            text: BASE_TEST_STRINGS.TEXT_WITH_NUMBERS,
            tokens: ["the", "company", "was", "founded", "in", "2016", "."],
            ids: [101, 1996, 2194, 2001, 2631, 1999, 2355, 1012, 102],
            decoded: "[CLS] the company was founded in 2016. [SEP]",
        },
        PUNCTUATION: {
            text: BASE_TEST_STRINGS.PUNCTUATION,
            tokens: ["a", "'", "ll", "!", "!", "to", "?", "'", "d", "'", "'", "d", "of", ",", "can", "'", "t", "."],
            ids: [101, 1037, 1005, 2222, 999, 999, 2000, 1029, 1005, 1040, 1005, 1005, 1040, 1997, 1010, 2064, 1005, 1056, 1012, 102],
            decoded: "[CLS] a'll!! to?'d'' d of, can't. [SEP]",
        },
        PYTHON_CODE: {
            text: BASE_TEST_STRINGS.PYTHON_CODE,
            tokens: ["def", "main", "(", ")", ":", "pass"],
            ids: [101, 13366, 2364, 1006, 1007, 1024, 3413, 102],
            decoded: "[CLS] def main ( ) : pass [SEP]",
        },
        JAVASCRIPT_CODE: {
            text: BASE_TEST_STRINGS.JAVASCRIPT_CODE,
            tokens: ["let", "a", "=", "ob", "##j", ".", "to", "##st", "##ring", "(", ")", ";", "to", "##st", "##ring", "(", ")", ";"],
            ids: [101, 2292, 1037, 1027, 27885, 3501, 1012, 2000, 3367, 4892, 1006, 1007, 1025, 2000, 3367, 4892, 1006, 1007, 1025, 102],
            decoded: "[CLS] let a = obj. tostring ( ) ; tostring ( ) ; [SEP]",
        },
        NEWLINES: {
            text: BASE_TEST_STRINGS.NEWLINES,
            tokens: ["this", "is", "a", "test", "."],
            ids: [101, 2023, 2003, 1037, 3231, 1012, 102],
            decoded: "[CLS] this is a test. [SEP]",
        },
        BASIC: {
            text: BASE_TEST_STRINGS.BASIC,
            tokens: ["unwanted", ",", "running"],
            ids: [101, 18162, 1010, 2770, 102],
            decoded: "[CLS] unwanted, running [SEP]",
        },
        CONTROL_TOKENS: {
            text: BASE_TEST_STRINGS.CONTROL_TOKENS,
            tokens: ["123"],
            ids: [101, 13138, 102],
            decoded: "[CLS] 123 [SEP]",
        },
        HELLO_WORLD_TITLECASE: {
            text: BASE_TEST_STRINGS.HELLO_WORLD_TITLECASE,
            tokens: ["hello", "world"],
            ids: [101, 7592, 2088, 102],
            decoded: "[CLS] hello world [SEP]",
        },
        HELLO_WORLD_LOWERCASE: {
            text: BASE_TEST_STRINGS.HELLO_WORLD_LOWERCASE,
            tokens: ["hello", "world"],
            ids: [101, 7592, 2088, 102],
            decoded: "[CLS] hello world [SEP]",
        },
        CHINESE_ONLY: {
            text: BASE_TEST_STRINGS.CHINESE_ONLY,
            tokens: ["\u751f", "[UNK]", "\u7684", "\u771f", "[UNK]", "[UNK]"],
            ids: [101, 1910, 100, 1916, 1921, 100, 100, 102],
            decoded: "[CLS] \u751f [UNK] \u7684 \u771f [UNK] [UNK] [SEP]",
        },
        LEADING_SPACE: {
            text: BASE_TEST_STRINGS.LEADING_SPACE,
            tokens: ["leading", "space"],
            ids: [101, 2877, 2686, 102],
            decoded: "[CLS] leading space [SEP]",
        },
        TRAILING_SPACE: {
            text: BASE_TEST_STRINGS.TRAILING_SPACE,
            tokens: ["trailing", "space"],
            ids: [101, 12542, 2686, 102],
            decoded: "[CLS] trailing space [SEP]",
        },
        DOUBLE_SPACE: {
            text: BASE_TEST_STRINGS.DOUBLE_SPACE,
            tokens: ["hi", "hello"],
            ids: [101, 7632, 7592, 102],
            decoded: "[CLS] hi hello [SEP]",
        },
        CURRENCY: {
            text: BASE_TEST_STRINGS.CURRENCY,
            tokens: ["test", "$", "1", "r", "##2", "#", "3", "\u20ac", "##4", "\u00a35", "\u00a5", "##6", "[UNK]", "\u20b9", "##8", "\u20b1", "##9", "test"],
            ids: [101, 3231, 1002, 1015, 1054, 2475, 1001, 1017, 1574, 2549, 27813, 1071, 2575, 100, 1576, 2620, 1575, 2683, 3231, 102],
            decoded: "[CLS] test $ 1 r2 # 3 \u20ac4 \u00a35 \u00a56 [UNK] \u20b98 \u20b19 test [SEP]",
        },
        CURRENCY_WITH_DECIMALS: {
            text: BASE_TEST_STRINGS.CURRENCY_WITH_DECIMALS,
            tokens: ["i", "bought", "an", "apple", "for", "$", "1", ".", "00", "at", "the", "store", "."],
            ids: [101, 1045, 4149, 2019, 6207, 2005, 1002, 1015, 1012, 4002, 2012, 1996, 3573, 1012, 102],
            decoded: "[CLS] i bought an apple for $ 1. 00 at the store. [SEP]",
        },
        ELLIPSIS: {
            text: BASE_TEST_STRINGS.ELLIPSIS,
            tokens: ["you", "\u2026"],
            ids: [101, 2017, 1529, 102],
            decoded: "[CLS] you \u2026 [SEP]",
        },
        TEXT_WITH_ESCAPE_CHARACTERS: {
            text: BASE_TEST_STRINGS.TEXT_WITH_ESCAPE_CHARACTERS,
            tokens: ["you", "\u2026"],
            ids: [101, 2017, 1529, 102],
            decoded: "[CLS] you \u2026 [SEP]",
        },
        TEXT_WITH_ESCAPE_CHARACTERS_2: {
            text: BASE_TEST_STRINGS.TEXT_WITH_ESCAPE_CHARACTERS_2,
            tokens: ["you", "\u2026", "you", "\u2026"],
            ids: [101, 2017, 1529, 2017, 1529, 102],
            decoded: "[CLS] you \u2026 you \u2026 [SEP]",
        },
        TILDE_NORMALIZATION: {
            text: BASE_TEST_STRINGS.TILDE_NORMALIZATION,
            tokens: ["weird", "\uff5e", "edge", "\uff5e", "case"],
            ids: [101, 6881, 1995, 3341, 1995, 2553, 102],
            decoded: "[CLS] weird \uff5e edge \uff5e case [SEP]",
        },
        SPIECE_UNDERSCORE: {
            text: BASE_TEST_STRINGS.SPIECE_UNDERSCORE,
            tokens: ["[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "."],
            ids: [101, 100, 100, 100, 100, 100, 1012, 102],
            decoded: "[CLS] [UNK] [UNK] [UNK] [UNK] [UNK]. [SEP]",
        },
        POPULAR_EMOJIS: {
            text: BASE_TEST_STRINGS.POPULAR_EMOJIS,
            tokens: ["[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]"],
            ids: [101, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 102],
            decoded: "[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]",
        },
        MULTIBYTE_EMOJIS: {
            text: BASE_TEST_STRINGS.MULTIBYTE_EMOJIS,
            tokens: ["[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]"],
            ids: [101, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 102],
            decoded: "[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]",
        },
        CHINESE_LATIN_MIXED: {
            text: BERT_TEST_STRINGS.CHINESE_LATIN_MIXED,
            tokens: ["ah", "\u535a", "[UNK]", "z", "##z"],
            ids: [101, 6289, 1786, 100, 1062, 2480, 102],
            decoded: "[CLS] ah \u535a [UNK] zz [SEP]",
        },
        SIMPLE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.SIMPLE_WITH_ACCENTS,
            tokens: ["hello"],
            ids: [101, 7592, 102],
            decoded: "[CLS] hello [SEP]",
        },
        MIXED_CASE_WITHOUT_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITHOUT_ACCENTS,
            tokens: ["hello", "!", "how", "are", "you", "?"],
            ids: [101, 7592, 999, 2129, 2024, 2017, 1029, 102],
            decoded: "[CLS] hello! how are you? [SEP]",
        },
        MIXED_CASE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITH_ACCENTS,
            tokens: ["hall", "##o", "!", "how", "are", "you", "?"],
            ids: [101, 2534, 2080, 999, 2129, 2024, 2017, 1029, 102],
            decoded: "[CLS] hallo! how are you? [SEP]",
        },
        ONLY_WHITESPACE: {
            text: BASE_TEST_STRINGS.ONLY_WHITESPACE,
            tokens: [],
            ids: [101, 102],
            decoded: "[CLS] [SEP]",
        },

        TEXT_PAIR: {
            text: "hello",
            text_pair: "world",
            tokens: ["hello", "world"],
            ids: [101, 7592, 102, 2088, 102],
            decoded: "[CLS] hello [SEP] world [SEP]",
        },
    },
    "Xenova/bert-base-cased": {
        JAVASCRIPT_CODE: {
            text: BASE_TEST_STRINGS.JAVASCRIPT_CODE,
            tokens: ["let", "a", "=", "o", "##b", "##j", ".", "to", "##S", "##tring", "(", ")", ";", "to", "##S", "##tring", "(", ")", ";"],
            ids: [101, 1519, 170, 134, 184, 1830, 3361, 119, 1106, 1708, 28108, 113, 114, 132, 1106, 1708, 28108, 113, 114, 132, 102],
            decoded: "[CLS] let a = obj. toString ( ) ; toString ( ) ; [SEP]",
        },
        BASIC: {
            text: BASE_TEST_STRINGS.BASIC,
            tokens: ["UN", "##wan", "##t\u00e9", "##d", ",", "running"],
            ids: [101, 7414, 5491, 14608, 1181, 117, 1919, 102],
            decoded: "[CLS] UNwant\u00e9d, running [SEP]",
        },
        CHINESE_ONLY: {
            text: BASE_TEST_STRINGS.CHINESE_ONLY,
            tokens: ["\u751f", "[UNK]", "[UNK]", "\u771f", "[UNK]", "[UNK]"],
            ids: [101, 1056, 100, 100, 1061, 100, 100, 102],
            decoded: "[CLS] \u751f [UNK] [UNK] \u771f [UNK] [UNK] [SEP]",
        },
        CURRENCY: {
            text: BASE_TEST_STRINGS.CURRENCY,
            tokens: ["test", "$", "1", "R", "##2", "#", "3", "\u20ac", "##4", "\u00a3", "##5", "\u00a5", "##6", "[UNK]", "\u20b9", "##8", "\u20b1", "##9", "test"],
            ids: [101, 2774, 109, 122, 155, 1477, 108, 124, 836, 1527, 202, 1571, 203, 1545, 100, 838, 1604, 837, 1580, 2774, 102],
            decoded: "[CLS] test $ 1 R2 # 3 \u20ac4 \u00a35 \u00a56 [UNK] \u20b98 \u20b19 test [SEP]",
        },
        CURRENCY_WITH_DECIMALS: {
            text: BASE_TEST_STRINGS.CURRENCY_WITH_DECIMALS,
            tokens: ["I", "bought", "an", "apple", "for", "$", "1", ".", "00", "at", "the", "store", "."],
            ids: [101, 146, 3306, 1126, 12075, 1111, 109, 122, 119, 3135, 1120, 1103, 2984, 119, 102],
            decoded: "[CLS] I bought an apple for $ 1. 00 at the store. [SEP]",
        },
        TILDE_NORMALIZATION: {
            text: BASE_TEST_STRINGS.TILDE_NORMALIZATION,
            tokens: ["weird", "[UNK]", "edge", "[UNK]", "case"],
            ids: [101, 6994, 100, 2652, 100, 1692, 102],
            decoded: "[CLS] weird [UNK] edge [UNK] case [SEP]",
        },
        CHINESE_LATIN_MIXED: {
            text: BERT_TEST_STRINGS.CHINESE_LATIN_MIXED,
            tokens: ["ah", "[UNK]", "[UNK]", "z", "##z"],
            ids: [101, 18257, 100, 100, 195, 1584, 102],
            decoded: "[CLS] ah [UNK] [UNK] zz [SEP]",
        },
        SIMPLE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.SIMPLE_WITH_ACCENTS,
            tokens: ["H", "##\u00e9", "##llo"],
            ids: [101, 145, 2744, 6643, 102],
            decoded: "[CLS] H\u00e9llo [SEP]",
        },
    },

    "Xenova/bert-base-multilingual-cased-ner-hrl": {
        SIMPLE: {
            text: BASE_TEST_STRINGS.SIMPLE,
            tokens: ["How", "are", "you", "doing", "?"],
            ids: [101, 14962, 10301, 13028, 30918, 136, 102],
            decoded: "[CLS] How are you doing? [SEP]",
        },
        SIMPLE_WITH_PUNCTUATION: {
            text: BASE_TEST_STRINGS.SIMPLE_WITH_PUNCTUATION,
            tokens: ["You", "should", "'", "ve", "done", "this"],
            ids: [101, 11065, 14819, 112, 10323, 20378, 10531, 102],
            decoded: "[CLS] You should've done this [SEP]",
        },
        TEXT_WITH_NUMBERS: {
            text: BASE_TEST_STRINGS.TEXT_WITH_NUMBERS,
            tokens: ["The", "company", "was", "founded", "in", "2016", "."],
            ids: [101, 10117, 12100, 10134, 14078, 10106, 10255, 119, 102],
            decoded: "[CLS] The company was founded in 2016. [SEP]",
        },
        PUNCTUATION: {
            text: BASE_TEST_STRINGS.PUNCTUATION,
            tokens: ["A", "'", "ll", "!", "!", "to", "?", "'", "d", "'", "'", "d", "of", ",", "can", "'", "t", "."],
            ids: [101, 138, 112, 22469, 106, 106, 10114, 136, 112, 172, 112, 112, 172, 10108, 117, 10944, 112, 188, 119, 102],
            decoded: "[CLS] A'll!! to?'d'' d of, can't. [SEP]",
        },
        JAVASCRIPT_CODE: {
            text: BASE_TEST_STRINGS.JAVASCRIPT_CODE,
            tokens: ["let", "a", "=", "ob", "##j", ".", "to", "##S", "##trin", "##g", "(", ")", ";", "to", "##S", "##trin", "##g", "(", ")", ";"],
            ids: [101, 13595, 169, 134, 17339, 10418, 119, 10114, 10731, 109163, 10240, 113, 114, 132, 10114, 10731, 109163, 10240, 113, 114, 132, 102],
            decoded: "[CLS] let a = obj. toString ( ) ; toString ( ) ; [SEP]",
        },
        NEWLINES: {
            text: BASE_TEST_STRINGS.NEWLINES,
            tokens: ["This", "is", "a", "test", "."],
            ids: [101, 10747, 10124, 169, 15839, 119, 102],
            decoded: "[CLS] This is a test. [SEP]",
        },
        BASIC: {
            text: BASE_TEST_STRINGS.BASIC,
            tokens: ["UN", "##want", "##\u00e9d", ",", "running"],
            ids: [101, 26578, 104216, 84193, 117, 18020, 102],
            decoded: "[CLS] UNwant\u00e9d, running [SEP]",
        },
        HELLO_WORLD_TITLECASE: {
            text: BASE_TEST_STRINGS.HELLO_WORLD_TITLECASE,
            tokens: ["Hello", "World"],
            ids: [101, 31178, 10315, 102],
            decoded: "[CLS] Hello World [SEP]",
        },
        HELLO_WORLD_LOWERCASE: {
            text: BASE_TEST_STRINGS.HELLO_WORLD_LOWERCASE,
            tokens: ["hell", "##o", "world"],
            ids: [101, 61694, 10133, 11356, 102],
            decoded: "[CLS] hello world [SEP]",
        },
        CHINESE_ONLY: {
            text: BASE_TEST_STRINGS.CHINESE_ONLY,
            tokens: ["\u751f", "\u6d3b", "\u7684", "\u771f", "\u8c1b", "\u662f"],
            ids: [101, 5600, 4978, 5718, 5769, 7378, 4380, 102],
            decoded: "[CLS] \u751f \u6d3b \u7684 \u771f \u8c1b \u662f [SEP]",
        },
        TRAILING_SPACE: {
            text: BASE_TEST_STRINGS.TRAILING_SPACE,
            tokens: ["trail", "##ing", "space"],
            ids: [101, 56559, 10230, 16199, 102],
            decoded: "[CLS] trailing space [SEP]",
        },
        DOUBLE_SPACE: {
            text: BASE_TEST_STRINGS.DOUBLE_SPACE,
            tokens: ["Hi", "Hello"],
            ids: [101, 20065, 31178, 102],
            decoded: "[CLS] Hi Hello [SEP]",
        },
        CURRENCY: {
            text: BASE_TEST_STRINGS.CURRENCY,
            tokens: ["test", "$", "1", "R2", "#", "3", "\u20ac", "##4", "\u00a3", "##5", "\u00a5", "##6", "[UNK]", "\u20b9", "##8", "[UNK]", "test"],
            ids: [101, 15839, 109, 122, 94000, 108, 124, 1775, 11011, 201, 11166, 202, 11211, 100, 1776, 11396, 100, 15839, 102],
            decoded: "[CLS] test $ 1 R2 # 3 \u20ac4 \u00a35 \u00a56 [UNK] \u20b98 [UNK] test [SEP]",
        },
        CURRENCY_WITH_DECIMALS: {
            text: BASE_TEST_STRINGS.CURRENCY_WITH_DECIMALS,
            tokens: ["I", "bought", "an", "app", "##le", "for", "$", "1", ".", "00", "at", "the", "store", "."],
            ids: [101, 146, 28870, 10151, 72894, 10284, 10142, 109, 122, 119, 11025, 10160, 10105, 13708, 119, 102],
            decoded: "[CLS] I bought an apple for $ 1. 00 at the store. [SEP]",
        },
        ELLIPSIS: {
            text: BASE_TEST_STRINGS.ELLIPSIS,
            tokens: ["you", "[UNK]"],
            ids: [101, 13028, 100, 102],
            decoded: "[CLS] you [UNK] [SEP]",
        },
        TEXT_WITH_ESCAPE_CHARACTERS: {
            text: BASE_TEST_STRINGS.TEXT_WITH_ESCAPE_CHARACTERS,
            tokens: ["you", "[UNK]"],
            ids: [101, 13028, 100, 102],
            decoded: "[CLS] you [UNK] [SEP]",
        },
        TEXT_WITH_ESCAPE_CHARACTERS_2: {
            text: BASE_TEST_STRINGS.TEXT_WITH_ESCAPE_CHARACTERS_2,
            tokens: ["you", "[UNK]", "you", "[UNK]"],
            ids: [101, 13028, 100, 13028, 100, 102],
            decoded: "[CLS] you [UNK] you [UNK] [SEP]",
        },
        TILDE_NORMALIZATION: {
            text: BASE_TEST_STRINGS.TILDE_NORMALIZATION,
            tokens: ["wei", "##rd", "\uff5e", "edge", "\uff5e", "case"],
            ids: [101, 86981, 12023, 10096, 30599, 10096, 13474, 102],
            decoded: "[CLS] weird \uff5e edge \uff5e case [SEP]",
        },
        CHINESE_LATIN_MIXED: {
            text: BERT_TEST_STRINGS.CHINESE_LATIN_MIXED,
            tokens: ["ah", "\u535a", "\u63a8", "z", "##z"],
            ids: [101, 69863, 2684, 4163, 194, 10305, 102],
            decoded: "[CLS] ah \u535a \u63a8 zz [SEP]",
        },
        SIMPLE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.SIMPLE_WITH_ACCENTS,
            tokens: ["H", "##\u00e9l", "##lo"],
            ids: [101, 145, 24817, 10715, 102],
            decoded: "[CLS] H\u00e9llo [SEP]",
        },
        MIXED_CASE_WITHOUT_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITHOUT_ACCENTS,
            tokens: ["He", "##LL", "##o", "!", "how", "Are", "yo", "##U", "?"],
            ids: [101, 10357, 82834, 10133, 106, 14796, 13491, 13672, 12022, 136, 102],
            decoded: "[CLS] HeLLo! how Are yoU? [SEP]",
        },
        MIXED_CASE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITH_ACCENTS,
            tokens: ["H", "##\u00e4", "##LL", "##o", "!", "how", "Are", "yo", "##U", "?"],
            ids: [101, 145, 11013, 82834, 10133, 106, 14796, 13491, 13672, 12022, 136, 102],
            decoded: "[CLS] H\u00e4LLo! how Are yoU? [SEP]",
        },
    },
    "Xenova/paraphrase-multilingual-MiniLM-L12-v2": {
        SIMPLE: {
            text: BASE_TEST_STRINGS.SIMPLE,
            tokens: ["\u2581How", "\u2581are", "\u2581you", "\u2581doing", "?"],
            ids: [0, 11249, 621, 398, 20594, 32, 2],
            decoded: "<s> How are you doing?</s>",
        },
        SIMPLE_WITH_PUNCTUATION: {
            text: BASE_TEST_STRINGS.SIMPLE_WITH_PUNCTUATION,
            tokens: ["\u2581You", "\u2581should", "'", "ve", "\u2581done", "\u2581this"],
            ids: [0, 2583, 5608, 25, 272, 16940, 903, 2],
            decoded: "<s> You should've done this</s>",
        },
        TEXT_WITH_NUMBERS: {
            text: BASE_TEST_STRINGS.TEXT_WITH_NUMBERS,
            tokens: ["\u2581The", "\u2581company", "\u2581was", "\u2581found", "ed", "\u2581in", "\u25812016."],
            ids: [0, 581, 14380, 509, 14037, 297, 23, 6360, 2],
            decoded: "<s> The company was founded in 2016.</s>",
        },
        PUNCTUATION: {
            text: BASE_TEST_STRINGS.PUNCTUATION,
            tokens: ["\u2581A", "\u2581'", "ll", "\u2581!!", "to", "?", "'", "d", "''", "d", "\u2581of", ",", "\u2581can", "'", "t", "."],
            ids: [0, 62, 242, 1181, 6506, 188, 32, 25, 71, 4765, 71, 111, 4, 831, 25, 18, 5, 2],
            decoded: "<s> A 'll!!to?'d''d of, can't.</s>",
        },
        PYTHON_CODE: {
            text: BASE_TEST_STRINGS.PYTHON_CODE,
            tokens: ["\u2581de", "f", "\u2581main", "(", "):", "\u2581pass"],
            ids: [0, 8, 420, 5201, 132, 2077, 27875, 2],
            decoded: "<s> def main(): pass</s>",
        },
        JAVASCRIPT_CODE: {
            text: BASE_TEST_STRINGS.JAVASCRIPT_CODE,
            tokens: ["\u2581let", "\u2581a", "\u2581=", "\u2581ob", "j", ".", "to", "Str", "ing", "(", ");", "\u2581to", "Str", "ing", "(", ");"],
            ids: [0, 2633, 10, 2203, 995, 170, 5, 188, 71713, 214, 132, 3142, 47, 71713, 214, 132, 3142, 2],
            decoded: "<s> let a = obj.toString(); toString();</s>",
        },
        NEWLINES: {
            text: BASE_TEST_STRINGS.NEWLINES,
            tokens: ["\u2581This", "\u2581is", "\u2581a", "\u2581test", "."],
            ids: [0, 3293, 83, 10, 3034, 5, 2],
            decoded: "<s> This is a test.</s>",
        },
        BASIC: {
            text: BASE_TEST_STRINGS.BASIC,
            tokens: ["\u2581UN", "wan", "t\u00e9", "d", ",", "run", "ning"],
            ids: [0, 8274, 3206, 2312, 71, 4, 16428, 592, 2],
            decoded: "<s> UNwant\u00e9d,running</s>",
        },
        CONTROL_TOKENS: {
            text: BASE_TEST_STRINGS.CONTROL_TOKENS,
            tokens: ["\u25811", "\u0000", "2", "\u25813"],
            ids: [0, 106, 3, 304, 138, 2],
            decoded: "<s> 1<unk>2 3</s>",
        },
        HELLO_WORLD_TITLECASE: {
            text: BASE_TEST_STRINGS.HELLO_WORLD_TITLECASE,
            tokens: ["\u2581Hello", "\u2581World"],
            ids: [0, 35378, 6661, 2],
            decoded: "<s> Hello World</s>",
        },
        HELLO_WORLD_LOWERCASE: {
            text: BASE_TEST_STRINGS.HELLO_WORLD_LOWERCASE,
            tokens: ["\u2581hell", "o", "\u2581world"],
            ids: [0, 33600, 31, 8999, 2],
            decoded: "<s> hello world</s>",
        },
        CHINESE_ONLY: {
            text: BASE_TEST_STRINGS.CHINESE_ONLY,
            tokens: ["\u2581", "\u751f\u6d3b\u7684", "\u771f", "\u8c1b", "\u662f"],
            ids: [0, 6, 62668, 5364, 245875, 354, 2],
            decoded: "<s> \u751f\u6d3b\u7684\u771f\u8c1b\u662f</s>",
        },
        LEADING_SPACE: {
            text: BASE_TEST_STRINGS.LEADING_SPACE,
            tokens: ["\u2581leading", "\u2581space"],
            ids: [0, 105207, 32628, 2],
            decoded: "<s> leading space</s>",
        },
        TRAILING_SPACE: {
            text: BASE_TEST_STRINGS.TRAILING_SPACE,
            tokens: ["\u2581trail", "ing", "\u2581space"],
            ids: [0, 141037, 214, 32628, 2],
            decoded: "<s> trailing space</s>",
        },
        DOUBLE_SPACE: {
            text: BASE_TEST_STRINGS.DOUBLE_SPACE,
            tokens: ["\u2581Hi", "\u2581Hello"],
            ids: [0, 2673, 35378, 2],
            decoded: "<s> Hi Hello</s>",
        },
        CURRENCY: {
            text: BASE_TEST_STRINGS.CURRENCY,
            tokens: ["\u2581test", "\u2581$1", "\u2581R", "2", "\u2581#3", "\u2581\u20ac", "4", "\u2581\u00a3", "5", "\u2581", "\u00a5", "6", "\u2581", "\u20a3", "7", "\u2581\u20b9", "8", "\u2581", "\u20b1", "9", "\u2581test"],
            ids: [0, 3034, 38629, 627, 304, 111378, 2505, 617, 11762, 758, 6, 32389, 910, 6, 3, 966, 87316, 1019, 6, 247425, 1126, 3034, 2],
            decoded: "<s> test $1 R2 #3 \u20ac4 \u00a35 \u00a56 <unk>7 \u20b98 \u20b19 test</s>",
        },
        CURRENCY_WITH_DECIMALS: {
            text: BASE_TEST_STRINGS.CURRENCY_WITH_DECIMALS,
            tokens: ["\u2581I", "\u2581bought", "\u2581an", "\u2581apple", "\u2581for", "\u2581$", "1.00", "\u2581at", "\u2581the", "\u2581store", "."],
            ids: [0, 87, 123997, 142, 108787, 100, 3650, 146533, 99, 70, 4343, 5, 2],
            decoded: "<s> I bought an apple for $1.00 at the store.</s>",
        },
        ELLIPSIS: {
            text: BASE_TEST_STRINGS.ELLIPSIS,
            tokens: ["\u2581you", "..."],
            ids: [0, 398, 27, 2],
            decoded: "<s> you...</s>",
        },
        TEXT_WITH_ESCAPE_CHARACTERS: {
            text: BASE_TEST_STRINGS.TEXT_WITH_ESCAPE_CHARACTERS,
            tokens: ["\u2581you", "..."],
            ids: [0, 398, 27, 2],
            decoded: "<s> you...</s>",
        },
        TEXT_WITH_ESCAPE_CHARACTERS_2: {
            text: BASE_TEST_STRINGS.TEXT_WITH_ESCAPE_CHARACTERS_2,
            tokens: ["\u2581you", "...", "\u2581you", "..."],
            ids: [0, 398, 27, 398, 27, 2],
            decoded: "<s> you... you...</s>",
        },
        TILDE_NORMALIZATION: {
            text: BASE_TEST_STRINGS.TILDE_NORMALIZATION,
            tokens: ["\u2581weird", "\u2581", "\uff5e", "\u2581edge", "\u2581", "\uff5e", "\u2581case"],
            ids: [0, 179459, 6, 6087, 121303, 6, 6087, 7225, 2],
            decoded: "<s> weird \uff5e edge \uff5e case</s>",
        },
        SPIECE_UNDERSCORE: {
            text: BASE_TEST_STRINGS.SPIECE_UNDERSCORE,
            tokens: ["\u2581This", "\u2581is", "\u2581a", "\u2581test", "\u2581", "."],
            ids: [0, 3293, 83, 10, 3034, 6, 5, 2],
            decoded: "<s> This is a test.</s>",
        },
        POPULAR_EMOJIS: {
            text: BASE_TEST_STRINGS.POPULAR_EMOJIS,
            tokens: ["\u2581", "\ud83d\ude02", "\u2581", "\ud83d\udc4d", "\u2581", "\ud83e\udd23", "\u2581", "\ud83d\ude0d", "\u2581", "\ud83d\ude2d", "\u2581", "\ud83c\udf89", "\u2581", "\ud83d\ude4f", "\u2581", "\ud83d\ude0a", "\u2581", "\ud83d\udd25", "\u2581", "\ud83d\ude01", "\u2581", "\ud83d\ude05", "\u2581", "\ud83e\udd17", "\u2581", "\ud83d\ude06", "\u2581", "\ud83d\udc4f", "\u2581\u2764", "\ufe0f", "\u2581", "\ud83d\udc9c", "\u2581", "\ud83d\udc9a", "\u2581", "\ud83d\udc97", "\u2581", "\ud83d\udc99", "\u2581", "\ud83d\udda4", "\u2581", "\ud83d\ude0e", "\u2581", "\ud83d\udc4c", "\u2581", "\ud83e\udd73", "\u2581", "\ud83d\udcaa", "\u2581", "\u2728", "\u2581", "\ud83d\udc49", "\u2581", "\ud83d\udc40", "\u2581", "\ud83d\udcaf", "\u2581", "\ud83c\udf88", "\u2581", "\ud83d\ude48", "\u2581", "\ud83d\ude4c", "\u2581", "\ud83d\udc80", "\u2581", "\ud83d\udc47", "\u2581", "\ud83d\udc4b", "\u2581", "\u2705", "\u2581", "\ud83c\udf81", "\u2581", "\ud83c\udf1e", "\u2581", "\ud83c\udf38", "\u2581", "\ud83d\udcb0"],
            ids: [0, 6, 115114, 6, 118280, 6, 243385, 6, 84464, 6, 232773, 6, 243816, 6, 113612, 6, 82803, 6, 222326, 6, 201344, 6, 239569, 6, 243544, 6, 191876, 6, 243404, 49933, 15755, 6, 244233, 6, 244162, 6, 244181, 6, 243892, 6, 245820, 6, 161546, 6, 204811, 6, 3, 6, 238992, 6, 167474, 6, 120242, 6, 245561, 6, 244864, 6, 246144, 6, 244459, 6, 244703, 6, 246887, 6, 144400, 6, 246511, 6, 142325, 6, 244230, 6, 245559, 6, 243374, 6, 245200, 2],
            decoded: "<s> \ud83d\ude02 \ud83d\udc4d \ud83e\udd23 \ud83d\ude0d \ud83d\ude2d \ud83c\udf89 \ud83d\ude4f \ud83d\ude0a \ud83d\udd25 \ud83d\ude01 \ud83d\ude05 \ud83e\udd17 \ud83d\ude06 \ud83d\udc4f \u2764\ufe0f \ud83d\udc9c \ud83d\udc9a \ud83d\udc97 \ud83d\udc99 \ud83d\udda4 \ud83d\ude0e \ud83d\udc4c <unk> \ud83d\udcaa \u2728 \ud83d\udc49 \ud83d\udc40 \ud83d\udcaf \ud83c\udf88 \ud83d\ude48 \ud83d\ude4c \ud83d\udc80 \ud83d\udc47 \ud83d\udc4b \u2705 \ud83c\udf81 \ud83c\udf1e \ud83c\udf38 \ud83d\udcb0</s>",
        },
        MULTIBYTE_EMOJIS: {
            text: BASE_TEST_STRINGS.MULTIBYTE_EMOJIS,
            tokens: ["\u2581", "\u2728", "\u2581", "\ud83e\udd17", "\u2581", "\ud83d\udc41", "\ufe0f", "\u2581", "\ud83d\udc71", "\ud83c\udffb", "\u2581", "\ud83d\udd75", "\u2581", "\u2642", "\ufe0f", "\u2581", "\ud83e\uddd9", "\ud83c\udffb", "\u2581", "\u2642", "\u2581", "\ud83d\udc68", "\ud83c\udffb", "\u2581", "\ud83c\udf3e", "\u2581", "\ud83e\uddd1", "\u2581", "\ud83e\udd1d", "\u2581", "\ud83e\uddd1", "\u2581", "\ud83d\udc69", "\u2581\u2764", "\u2581", "\ud83d\udc8b", "\u2581", "\ud83d\udc68", "\u2581", "\ud83d\udc69", "\u2581", "\ud83d\udc69", "\u2581", "\ud83d\udc67", "\u2581", "\ud83d\udc66", "\u2581", "\ud83e\uddd1", "\ud83c\udffb", "\u2581", "\ud83e\udd1d", "\u2581", "\ud83e\uddd1", "\ud83c\udffb", "\u2581", "\ud83c\udff4\udb40\udc67\udb40\udc62\udb40\udc65\udb40\udc6e\udb40\udc67\udb40\udc7f", "\u2581", "\ud83d\udc68", "\ud83c\udffb", "\u2581\u2764", "\ufe0f", "\u2581", "\ud83d\udc8b", "\u2581", "\ud83d\udc68", "\ud83c\udffc"],
            ids: [0, 6, 167474, 6, 243544, 6, 246984, 15755, 6, 247201, 79500, 6, 248325, 6, 228250, 15755, 6, 3, 79500, 6, 228250, 6, 244314, 79500, 6, 246529, 6, 3, 6, 247443, 6, 3, 6, 244785, 49933, 6, 244960, 6, 244314, 6, 244785, 6, 244785, 6, 245719, 6, 246167, 6, 3, 79500, 6, 247443, 6, 3, 79500, 6, 3, 6, 244314, 79500, 49933, 15755, 6, 244960, 6, 244314, 239719, 2],
            decoded: "<s> \u2728 \ud83e\udd17 \ud83d\udc41\ufe0f \ud83d\udc71\ud83c\udffb \ud83d\udd75 \u2642\ufe0f <unk>\ud83c\udffb \u2642 \ud83d\udc68\ud83c\udffb \ud83c\udf3e <unk> \ud83e\udd1d <unk> \ud83d\udc69 \u2764 \ud83d\udc8b \ud83d\udc68 \ud83d\udc69 \ud83d\udc69 \ud83d\udc67 \ud83d\udc66 <unk>\ud83c\udffb \ud83e\udd1d <unk>\ud83c\udffb <unk> \ud83d\udc68\ud83c\udffb \u2764\ufe0f \ud83d\udc8b \ud83d\udc68\ud83c\udffc</s>",
        },
        CHINESE_LATIN_MIXED: {
            text: BERT_TEST_STRINGS.CHINESE_LATIN_MIXED,
            tokens: ["\u2581ah", "\u535a", "\u63a8", "zz"],
            ids: [0, 1263, 11173, 10238, 13894, 2],
            decoded: "<s> ah\u535a\u63a8zz</s>",
        },
        SIMPLE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.SIMPLE_WITH_ACCENTS,
            tokens: ["\u2581H\u00e9", "llo"],
            ids: [0, 88064, 9284, 2],
            decoded: "<s> H\u00e9llo</s>",
        },
        MIXED_CASE_WITHOUT_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITHOUT_ACCENTS,
            tokens: ["\u2581He", "LL", "o", "!", "how", "\u2581Are", "\u2581yo", "U", "?"],
            ids: [0, 1529, 23708, 31, 38, 47251, 15901, 3005, 1062, 32, 2],
            decoded: "<s> HeLLo!how Are yoU?</s>",
        },
        MIXED_CASE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITH_ACCENTS,
            tokens: ["\u2581H\u00e4", "LL", "o", "!", "how", "\u2581Are", "\u2581yo", "U", "?"],
            ids: [0, 28863, 23708, 31, 38, 47251, 15901, 3005, 1062, 32, 2],
            decoded: "<s> H\u00e4LLo!how Are yoU?</s>",
        },
    },
    "Xenova/bert-base-multilingual-uncased-sentiment": {
        JAVASCRIPT_CODE: {
            text: BASE_TEST_STRINGS.JAVASCRIPT_CODE,
            tokens: ["let", "a", "=", "ob", "##j", ".", "tos", "##tri", "##ng", "(", ")", ";", "tos", "##tri", "##ng", "(", ")", ";"],
            ids: [101, 12421, 143, 134, 15547, 10428, 119, 53564, 27711, 10422, 113, 114, 132, 53564, 27711, 10422, 113, 114, 132, 102],
            decoded: "[CLS] let a = obj. tostring ( ) ; tostring ( ) ; [SEP]",
        },
        BASIC: {
            text: BASE_TEST_STRINGS.BASIC,
            tokens: ["un", "##wan", "##ted", ",", "running"],
            ids: [101, 10119, 15134, 11894, 117, 16484, 102],
            decoded: "[CLS] unwanted, running [SEP]",
        },
        CURRENCY: {
            text: BASE_TEST_STRINGS.CURRENCY,
            tokens: ["test", "$", "1", "r2", "#", "3", "\u20ac", "##4", "\u00a3", "##5", "\u00a5", "##6", "[UNK]", "\u20b9", "##8", "\u20b1", "##9", "test"],
            ids: [101, 14084, 109, 122, 85583, 108, 124, 1329, 11124, 175, 11301, 177, 11325, 100, 1332, 11544, 1330, 11518, 14084, 102],
            decoded: "[CLS] test $ 1 r2 # 3 \u20ac4 \u00a35 \u00a56 [UNK] \u20b98 \u20b19 test [SEP]",
        },
    },
    "Xenova/multilingual-e5-small": {
        TRAILING_SPACE: {
            text: BASE_TEST_STRINGS.TRAILING_SPACE,
            tokens: ["\u2581trail", "ing", "\u2581space", "\u2581"],
            ids: [0, 141037, 214, 32628, 6, 2],
            decoded: "<s> trailing space </s>",
        },
        ELLIPSIS: {
            text: BASE_TEST_STRINGS.ELLIPSIS,
            tokens: ["\u2581you", "...", "\u2581"],
            ids: [0, 398, 27, 6, 2],
            decoded: "<s> you... </s>",
        },
        TEXT_WITH_ESCAPE_CHARACTERS: {
            text: BASE_TEST_STRINGS.TEXT_WITH_ESCAPE_CHARACTERS,
            tokens: ["\u2581you", "...", "\u2581"],
            ids: [0, 398, 27, 6, 2],
            decoded: "<s> you... </s>",
        },
        TEXT_WITH_ESCAPE_CHARACTERS_2: {
            text: BASE_TEST_STRINGS.TEXT_WITH_ESCAPE_CHARACTERS_2,
            tokens: ["\u2581you", "...", "\u2581you", "...", "\u2581"],
            ids: [0, 398, 27, 398, 27, 6, 2],
            decoded: "<s> you... you... </s>",
        },
        MIXED_CASE_WITHOUT_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITHOUT_ACCENTS,
            tokens: ["\u2581He", "LL", "o", "!", "how", "\u2581Are", "\u2581yo", "U", "?", "\u2581"],
            ids: [0, 1529, 23708, 31, 38, 47251, 15901, 3005, 1062, 32, 6, 2],
            decoded: "<s> HeLLo!how Are yoU? </s>",
        },
        MIXED_CASE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITH_ACCENTS,
            tokens: ["\u2581H\u00e4", "LL", "o", "!", "how", "\u2581Are", "\u2581yo", "U", "?", "\u2581"],
            ids: [0, 28863, 23708, 31, 38, 47251, 15901, 3005, 1062, 32, 6, 2],
            decoded: "<s> H\u00e4LLo!how Are yoU? </s>",
        },
    },
    "Xenova/bge-small-zh-v1.5": {
        SIMPLE: {
            text: BASE_TEST_STRINGS.SIMPLE,
            tokens: ["[UNK]", "are", "you", "doi", "##ng", "?"],
            ids: [101, 100, 8995, 8357, 9962, 8291, 136, 102],
            decoded: "[CLS] [UNK] are you doing? [SEP]",
        },
        SIMPLE_WITH_PUNCTUATION: {
            text: BASE_TEST_STRINGS.SIMPLE_WITH_PUNCTUATION,
            tokens: ["[UNK]", "sh", "##ould", "'", "ve", "don", "##e", "this"],
            ids: [101, 100, 11167, 11734, 112, 12810, 9524, 8154, 8554, 102],
            decoded: "[CLS] [UNK] should've done this [SEP]",
        },
        TEXT_WITH_NUMBERS: {
            text: BASE_TEST_STRINGS.TEXT_WITH_NUMBERS,
            tokens: ["[UNK]", "company", "was", "f", "##ound", "##ed", "in", "2016", "."],
            ids: [101, 100, 10007, 9947, 148, 11477, 8303, 8217, 8112, 119, 102],
            decoded: "[CLS] [UNK] company was founded in 2016. [SEP]",
        },
        PUNCTUATION: {
            text: BASE_TEST_STRINGS.PUNCTUATION,
            tokens: ["[UNK]", "'", "ll", "!", "!", "to", "?", "'", "d", "'", "'", "d", "of", ",", "can", "'", "t", "."],
            ids: [101, 100, 112, 10856, 106, 106, 8228, 136, 112, 146, 112, 112, 146, 8205, 117, 9109, 112, 162, 119, 102],
            decoded: "[CLS] [UNK]'ll!! to?'d'' d of, can't. [SEP]",
        },
        PYTHON_CODE: {
            text: BASE_TEST_STRINGS.PYTHON_CODE,
            tokens: ["de", "##f", "main", "(", ")", ":", "pass"],
            ids: [101, 8363, 8189, 9139, 113, 114, 131, 9703, 102],
            decoded: "[CLS] def main ( ) : pass [SEP]",
        },
        JAVASCRIPT_CODE: {
            text: BASE_TEST_STRINGS.JAVASCRIPT_CODE,
            tokens: ["let", "a", "=", "ob", "##j", ".", "[UNK]", "(", ")", ";", "[UNK]", "(", ")", ";"],
            ids: [101, 9946, 143, 134, 12639, 8334, 119, 100, 113, 114, 132, 100, 113, 114, 132, 102],
            decoded: "[CLS] let a = obj. [UNK] ( ) ; [UNK] ( ) ; [SEP]",
        },
        NEWLINES: {
            text: BASE_TEST_STRINGS.NEWLINES,
            tokens: ["[UNK]", "is", "a", "test", "."],
            ids: [101, 100, 8310, 143, 10060, 119, 102],
            decoded: "[CLS] [UNK] is a test. [SEP]",
        },
        BASIC: {
            text: BASE_TEST_STRINGS.BASIC,
            tokens: ["[UNK]", ",", "running"],
            ids: [101, 100, 117, 11620, 102],
            decoded: "[CLS] [UNK], running [SEP]",
        },
        HELLO_WORLD_TITLECASE: {
            text: BASE_TEST_STRINGS.HELLO_WORLD_TITLECASE,
            tokens: ["[UNK]", "[UNK]"],
            ids: [101, 100, 100, 102],
            decoded: "[CLS] [UNK] [UNK] [SEP]",
        },
        LEADING_SPACE: {
            text: BASE_TEST_STRINGS.LEADING_SPACE,
            tokens: ["le", "##ad", "##ing", "space"],
            ids: [101, 8983, 8695, 8221, 9634, 102],
            decoded: "[CLS] leading space [SEP]",
        },
        TRAILING_SPACE: {
            text: BASE_TEST_STRINGS.TRAILING_SPACE,
            tokens: ["t", "##rail", "##ing", "space"],
            ids: [101, 162, 12783, 8221, 9634, 102],
            decoded: "[CLS] trailing space [SEP]",
        },
        DOUBLE_SPACE: {
            text: BASE_TEST_STRINGS.DOUBLE_SPACE,
            tokens: ["[UNK]", "[UNK]"],
            ids: [101, 100, 100, 102],
            decoded: "[CLS] [UNK] [UNK] [SEP]",
        },
        CURRENCY: {
            text: BASE_TEST_STRINGS.CURRENCY,
            tokens: ["test", "$", "1", "[UNK]", "#", "3", "\u20ac", "##4", "\u00a3", "##5", "\u00a5", "##6", "[UNK]", "[UNK]", "[UNK]", "test"],
            ids: [101, 10060, 109, 122, 100, 108, 124, 359, 8159, 173, 8157, 175, 8158, 100, 100, 100, 10060, 102],
            decoded: "[CLS] test $ 1 [UNK] # 3 \u20ac4 \u00a35 \u00a56 [UNK] [UNK] [UNK] test [SEP]",
        },
        CURRENCY_WITH_DECIMALS: {
            text: BASE_TEST_STRINGS.CURRENCY_WITH_DECIMALS,
            tokens: ["[UNK]", "bo", "##ugh", "##t", "an", "apple", "for", "$", "1", ".", "00", "at", "the", "store", "."],
            ids: [101, 100, 11059, 12667, 8165, 9064, 8350, 8330, 109, 122, 119, 8136, 8243, 8174, 8719, 119, 102],
            decoded: "[CLS] [UNK] bought an apple for $ 1. 00 at the store. [SEP]",
        },
        POPULAR_EMOJIS: {
            text: BASE_TEST_STRINGS.POPULAR_EMOJIS,
            tokens: ["\ud83d\ude02", "\ud83d\udc4d", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "\ud83d\udd25", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "\ud83d\ude0e", "[UNK]", "[UNK]", "[UNK]", "\u2728", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]"],
            ids: [101, 8104, 8102, 100, 100, 100, 100, 100, 100, 8103, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 8105, 100, 100, 100, 501, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 102],
            decoded: "[CLS] \ud83d\ude02 \ud83d\udc4d [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] \ud83d\udd25 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] \ud83d\ude0e [UNK] [UNK] [UNK] \u2728 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]",
        },
        MULTIBYTE_EMOJIS: {
            text: BASE_TEST_STRINGS.MULTIBYTE_EMOJIS,
            tokens: ["\u2728", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]"],
            ids: [101, 501, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 102],
            decoded: "[CLS] \u2728 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]",
        },
        SIMPLE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.SIMPLE_WITH_ACCENTS,
            tokens: ["[UNK]"],
            ids: [101, 100, 102],
            decoded: "[CLS] [UNK] [SEP]",
        },
        MIXED_CASE_WITHOUT_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITHOUT_ACCENTS,
            tokens: ["[UNK]", "!", "how", "[UNK]", "[UNK]", "?"],
            ids: [101, 100, 106, 9510, 100, 100, 136, 102],
            decoded: "[CLS] [UNK]! how [UNK] [UNK]? [SEP]",
        },
        MIXED_CASE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITH_ACCENTS,
            tokens: ["[UNK]", "!", "how", "[UNK]", "[UNK]", "?"],
            ids: [101, 100, 106, 9510, 100, 100, 136, 102],
            decoded: "[CLS] [UNK]! how [UNK] [UNK]? [SEP]",
        },
    },
    "Xenova/bge-base-zh-v1.5": {
        SIMPLE: {
            text: BASE_TEST_STRINGS.SIMPLE,
            tokens: ["how", "are", "you", "doi", "##ng", "?"],
            ids: [101, 9510, 8995, 8357, 9962, 8291, 136, 102],
            decoded: "[CLS] how are you doing? [SEP]",
        },
        SIMPLE_WITH_PUNCTUATION: {
            text: BASE_TEST_STRINGS.SIMPLE_WITH_PUNCTUATION,
            tokens: ["you", "sh", "##ould", "'", "ve", "don", "##e", "this"],
            ids: [101, 8357, 11167, 11734, 112, 12810, 9524, 8154, 8554, 102],
            decoded: "[CLS] you should've done this [SEP]",
        },
        TEXT_WITH_NUMBERS: {
            text: BASE_TEST_STRINGS.TEXT_WITH_NUMBERS,
            tokens: ["the", "company", "was", "f", "##ound", "##ed", "in", "2016", "."],
            ids: [101, 8174, 10007, 9947, 148, 11477, 8303, 8217, 8112, 119, 102],
            decoded: "[CLS] the company was founded in 2016. [SEP]",
        },
        BASIC: {
            text: BASE_TEST_STRINGS.BASIC,
            tokens: ["u", "##n", "##wan", "##ted", ",", "running"],
            ids: [101, 163, 8171, 9951, 9255, 117, 11620, 102],
            decoded: "[CLS] unwanted, running [SEP]",
        },
        CURRENCY: {
            text: BASE_TEST_STRINGS.CURRENCY,
            tokens: ["test", "$", "1", "r2", "#", "3", "\u20ac", "##4", "\u00a3", "##5", "\u00a5", "##6", "[UNK]", "[UNK]", "[UNK]", "test"],
            ids: [101, 10060, 109, 122, 11345, 108, 124, 359, 8159, 173, 8157, 175, 8158, 100, 100, 100, 10060, 102],
            decoded: "[CLS] test $ 1 r2 # 3 \u20ac4 \u00a35 \u00a56 [UNK] [UNK] [UNK] test [SEP]",
        },
        CURRENCY_WITH_DECIMALS: {
            text: BASE_TEST_STRINGS.CURRENCY_WITH_DECIMALS,
            tokens: ["i", "bo", "##ugh", "##t", "an", "apple", "for", "$", "1", ".", "00", "at", "the", "store", "."],
            ids: [101, 151, 11059, 12667, 8165, 9064, 8350, 8330, 109, 122, 119, 8136, 8243, 8174, 8719, 119, 102],
            decoded: "[CLS] i bought an apple for $ 1. 00 at the store. [SEP]",
        },
        POPULAR_EMOJIS: {
            text: BASE_TEST_STRINGS.POPULAR_EMOJIS,
            tokens: ["\ud83d\ude02", "\ud83d\udc4d", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "\ud83d\udd25", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "\u2764", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "\ud83d\ude0e", "[UNK]", "[UNK]", "[UNK]", "\u2728", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]"],
            ids: [101, 8104, 8102, 100, 100, 100, 100, 100, 100, 8103, 100, 100, 100, 100, 100, 506, 100, 100, 100, 100, 100, 8105, 100, 100, 100, 501, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 102],
            decoded: "[CLS] \ud83d\ude02 \ud83d\udc4d [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] \ud83d\udd25 [UNK] [UNK] [UNK] [UNK] [UNK] \u2764 [UNK] [UNK] [UNK] [UNK] [UNK] \ud83d\ude0e [UNK] [UNK] [UNK] \u2728 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]",
        },
    },
    "Xenova/indobert-base-p1": {
        SIMPLE_WITH_PUNCTUATION: {
            text: BASE_TEST_STRINGS.SIMPLE_WITH_PUNCTUATION,
            tokens: ["you", "sho", "##uld", "'", "ve", "don", "##e", "this"],
            ids: [2, 3299, 9596, 15370, 30463, 28239, 4081, 30357, 5379, 3],
            decoded: "[CLS] you should've done this [SEP]",
        },
        TEXT_WITH_NUMBERS: {
            text: BASE_TEST_STRINGS.TEXT_WITH_NUMBERS,
            tokens: ["the", "company", "was", "found", "##ed", "in", "2016", "."],
            ids: [2, 1002, 9105, 2738, 11009, 133, 48, 1538, 30470, 3],
            decoded: "[CLS] the company was founded in 2016. [SEP]",
        },
        JAVASCRIPT_CODE: {
            text: BASE_TEST_STRINGS.JAVASCRIPT_CODE,
            tokens: ["let", "a", "=", "ob", "##j", ".", "tos", "##trin", "##g", "(", ")", ";", "tos", "##trin", "##g", "(", ")", ";"],
            ids: [2, 4734, 253, 30475, 559, 30372, 30470, 20498, 12448, 30365, 30464, 30465, 30473, 20498, 12448, 30365, 30464, 30465, 30473, 3],
            decoded: "[CLS] let a = obj. tostring ( ) ; tostring ( ) ; [SEP]",
        },
        BASIC: {
            text: BASE_TEST_STRINGS.BASIC,
            tokens: ["un", "##wan", "##te", "##d", ",", "running"],
            ids: [2, 78, 1322, 3298, 30364, 30468, 22715, 3],
            decoded: "[CLS] unwanted, running [SEP]",
        },
        CHINESE_ONLY: {
            text: BASE_TEST_STRINGS.CHINESE_ONLY,
            tokens: ["[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]"],
            ids: [2, 1, 1, 1, 1, 1, 1, 3],
            decoded: "[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]",
        },
        LEADING_SPACE: {
            text: BASE_TEST_STRINGS.LEADING_SPACE,
            tokens: ["lead", "##ing", "space"],
            ids: [2, 9196, 55, 14561, 3],
            decoded: "[CLS] leading space [SEP]",
        },
        CURRENCY: {
            text: BASE_TEST_STRINGS.CURRENCY,
            tokens: ["test", "$", "1", "r", "##2", "#", "3", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "test"],
            ids: [2, 4243, 30460, 111, 56, 30378, 30459, 283, 1, 1, 1, 1, 1, 1, 4243, 3],
            decoded: "[CLS] test $ 1 r2 # 3 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] test [SEP]",
        },
        CURRENCY_WITH_DECIMALS: {
            text: BASE_TEST_STRINGS.CURRENCY_WITH_DECIMALS,
            tokens: ["i", "bo", "##ught", "an", "apple", "for", "$", "1", ".", "00", "at", "the", "store", "."],
            ids: [2, 89, 1880, 25009, 223, 7761, 1548, 30460, 111, 30470, 4230, 117, 1002, 8052, 30470, 3],
            decoded: "[CLS] i bought an apple for $ 1. 00 at the store. [SEP]",
        },
        TILDE_NORMALIZATION: {
            text: BASE_TEST_STRINGS.TILDE_NORMALIZATION,
            tokens: ["wei", "##rd", "[UNK]", "edge", "[UNK]", "case"],
            ids: [2, 27753, 12548, 1, 21418, 1, 13687, 3],
            decoded: "[CLS] weird [UNK] edge [UNK] case [SEP]",
        },
        MIXED_CASE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITH_ACCENTS,
            tokens: ["hallo", "!", "how", "are", "you", "?"],
            ids: [2, 19598, 30457, 11088, 5811, 3299, 30477, 3],
            decoded: "[CLS] hallo! how are you? [SEP]",
        },
    },
    "Xenova/spanbert-large-cased": {
        JAVASCRIPT_CODE: {
            text: BASE_TEST_STRINGS.JAVASCRIPT_CODE,
            tokens: ["let", "a", "=", "o", "##b", "##j", ".", "to", "##st", "##ring", "(", ")", ";", "to", "##st", "##ring", "(", ")", ";"],
            ids: [101, 1519, 170, 134, 184, 1830, 3361, 119, 1106, 2050, 3384, 113, 114, 132, 1106, 2050, 3384, 113, 114, 132, 102],
            decoded: "[CLS] let a = obj. tostring ( ) ; tostring ( ) ; [SEP]",
        },
        CURRENCY: {
            text: BASE_TEST_STRINGS.CURRENCY,
            tokens: ["test", "$", "1", "r", "##2", "#", "3", "\u20ac", "##4", "\u00a3", "##5", "\u00a5", "##6", "[UNK]", "\u20b9", "##8", "\u20b1", "##9", "test"],
            ids: [101, 2774, 109, 122, 187, 1477, 108, 124, 836, 1527, 202, 1571, 203, 1545, 100, 838, 1604, 837, 1580, 2774, 102],
            decoded: "[CLS] test $ 1 r2 # 3 \u20ac4 \u00a35 \u00a56 [UNK] \u20b98 \u20b19 test [SEP]",
        },
    },
    "Xenova/UMLSBert_ENG": {
        JAVASCRIPT_CODE: {
            text: BASE_TEST_STRINGS.JAVASCRIPT_CODE,
            tokens: ["let", "a", "=", "obj", ".", "tos", "##tring", "(", ")", ";", "tos", "##tring", "(", ")", ";"],
            ids: [2, 8894, 42, 32, 2473, 17, 22660, 23640, 11, 12, 30, 22660, 23640, 11, 12, 30, 3],
            decoded: "[CLS] let a = obj. tostring ( ) ; tostring ( ) ; [SEP]",
        },
        HELLO_WORLD_TITLECASE: {
            text: BASE_TEST_STRINGS.HELLO_WORLD_TITLECASE,
            tokens: ["hel", "##lo", "world"],
            ids: [2, 3018, 5368, 4517, 3],
            decoded: "[CLS] hello world [SEP]",
        },
        HELLO_WORLD_LOWERCASE: {
            text: BASE_TEST_STRINGS.HELLO_WORLD_LOWERCASE,
            tokens: ["hel", "##lo", "world"],
            ids: [2, 3018, 5368, 4517, 3],
            decoded: "[CLS] hello world [SEP]",
        },
        DOUBLE_SPACE: {
            text: BASE_TEST_STRINGS.DOUBLE_SPACE,
            tokens: ["hi", "hel", "##lo"],
            ids: [2, 11245, 3018, 5368, 3],
            decoded: "[CLS] hi hello [SEP]",
        },
        CURRENCY: {
            text: BASE_TEST_STRINGS.CURRENCY,
            tokens: ["test", "$", "1", "r2", "#", "3", "\u20ac", "##4", "\u00a3", "##5", "\u00a5", "##6", "\u20a3", "##7", "\u20b9", "##8", "\u20b1", "##9", "test"],
            ids: [2, 2313, 7, 20, 9663, 6, 22, 528, 1017, 74, 1009, 76, 1018, 524, 1019, 531, 1011, 529, 1038, 2313, 3],
            decoded: "[CLS] test $ 1 r2 # 3 \u20ac4 \u00a35 \u00a56 \u20a37 \u20b98 \u20b19 test [SEP]",
        },
        TILDE_NORMALIZATION: {
            text: BASE_TEST_STRINGS.TILDE_NORMALIZATION,
            tokens: ["we", "##ir", "##d", "\uff5e", "edge", "\uff5e", "case"],
            ids: [2, 1802, 1753, 1022, 943, 9676, 943, 2632, 3],
            decoded: "[CLS] weird \uff5e edge \uff5e case [SEP]",
        },
        SIMPLE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.SIMPLE_WITH_ACCENTS,
            tokens: ["hel", "##lo"],
            ids: [2, 3018, 5368, 3],
            decoded: "[CLS] hello [SEP]",
        },
        MIXED_CASE_WITHOUT_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITHOUT_ACCENTS,
            tokens: ["hel", "##lo", "!", "how", "are", "you", "?"],
            ids: [2, 3018, 5368, 5, 2135, 1810, 17915, 34, 3],
            decoded: "[CLS] hello! how are you? [SEP]",
        },
    },
    "Xenova/SapBERT-from-PubMedBERT-fulltext": {
        CHINESE_ONLY: {
            text: BASE_TEST_STRINGS.CHINESE_ONLY,
            tokens: ["\u751f", "\u6d3b", "\u7684", "[UNK]", "[UNK]", "\u662f"],
            ids: [2, 799, 776, 811, 1, 1, 731, 3],
            decoded: "[CLS] \u751f \u6d3b \u7684 [UNK] [UNK] \u662f [SEP]",
        },
        CURRENCY: {
            text: BASE_TEST_STRINGS.CURRENCY,
            tokens: ["test", "$", "1", "r2", "#", "3", "\u20ac", "##4", "\u00a3", "##5", "\u00a5", "##6", "[UNK]", "\u20b9", "##8", "[UNK]", "test"],
            ids: [2, 2648, 8, 21, 7261, 7, 23, 281, 1006, 76, 1015, 78, 1016, 1, 282, 1025, 1, 2648, 3],
            decoded: "[CLS] test $ 1 r2 # 3 \u20ac4 \u00a35 \u00a56 [UNK] \u20b98 [UNK] test [SEP]",
        },
    },
    "Xenova/rubert-base-cased": {
        SIMPLE: {
            text: BASE_TEST_STRINGS.SIMPLE,
            tokens: ["How", "are", "you", "do", "##ing", "?"],
            ids: [101, 15474, 10813, 13540, 10661, 7729, 166, 102],
            decoded: "[CLS] How are you doing? [SEP]",
        },
        SIMPLE_WITH_PUNCTUATION: {
            text: BASE_TEST_STRINGS.SIMPLE_WITH_PUNCTUATION,
            tokens: ["You", "sh", "##oul", "##d", "'", "ve", "don", "##e", "this"],
            ids: [101, 11577, 45942, 76143, 239, 118, 10835, 17450, 241, 11043, 102],
            decoded: "[CLS] You should've done this [SEP]",
        },
        TEXT_WITH_NUMBERS: {
            text: BASE_TEST_STRINGS.TEXT_WITH_NUMBERS,
            tokens: ["The", "comp", "##any", "was", "f", "##ound", "##ed", "in", "2016", "."],
            ids: [101, 6821, 71382, 17927, 10646, 242, 71129, 7491, 10618, 8273, 132, 102],
            decoded: "[CLS] The company was founded in 2016. [SEP]",
        },
        JAVASCRIPT_CODE: {
            text: BASE_TEST_STRINGS.JAVASCRIPT_CODE,
            tokens: ["let", "a", "=", "ob", "##j", ".", "to", "##St", "##ring", "(", ")", ";", "to", "##St", "##ring", "(", ")", ";"],
            ids: [101, 14107, 232, 162, 17851, 251, 132, 10626, 21568, 13647, 120, 122, 158, 10626, 21568, 13647, 120, 122, 158, 102],
            decoded: "[CLS] let a = obj. toString ( ) ; toString ( ) ; [SEP]",
        },
        BASIC: {
            text: BASE_TEST_STRINGS.BASIC,
            tokens: ["UN", "##wan", "##t", "##\u00e9d", ",", "run", "##ning"],
            ids: [101, 27090, 14906, 271, 84705, 128, 14607, 11781, 102],
            decoded: "[CLS] UNwant\u00e9d, running [SEP]",
        },
        CHINESE_ONLY: {
            text: BASE_TEST_STRINGS.CHINESE_ONLY,
            tokens: ["\u751f", "\u6d3b", "\u7684", "\u771f", "[UNK]", "\u662f"],
            ids: [101, 6104, 5480, 6222, 6273, 100, 4877, 102],
            decoded: "[CLS] \u751f \u6d3b \u7684 \u771f [UNK] \u662f [SEP]",
        },
        LEADING_SPACE: {
            text: BASE_TEST_STRINGS.LEADING_SPACE,
            tokens: ["le", "##ading", "sp", "##ace"],
            ids: [101, 10653, 73130, 33162, 13967, 102],
            decoded: "[CLS] leading space [SEP]",
        },
        TRAILING_SPACE: {
            text: BASE_TEST_STRINGS.TRAILING_SPACE,
            tokens: ["tra", "##ili", "##ng", "sp", "##ace"],
            ids: [101, 11776, 14296, 10888, 33162, 13967, 102],
            decoded: "[CLS] trailing space [SEP]",
        },
        CURRENCY_WITH_DECIMALS: {
            text: BASE_TEST_STRINGS.CURRENCY_WITH_DECIMALS,
            tokens: ["I", "bo", "##ught", "an", "app", "##le", "for", "$", "1", ".", "00", "at", "the", "st", "##ore", "."],
            ids: [101, 186, 21018, 53718, 10663, 73406, 7159, 10654, 112, 138, 132, 11537, 10672, 10617, 28668, 13536, 132, 102],
            decoded: "[CLS] I bought an apple for $ 1. 00 at the store. [SEP]",
        },
        TILDE_NORMALIZATION: {
            text: BASE_TEST_STRINGS.TILDE_NORMALIZATION,
            tokens: ["we", "##ird", "\uff5e", "ed", "##ge", "\uff5e", "cas", "##e"],
            ids: [101, 12463, 36865, 10608, 11051, 11037, 10608, 15501, 241, 102],
            decoded: "[CLS] weird \uff5e edge \uff5e case [SEP]",
        },
        CHINESE_LATIN_MIXED: {
            text: BERT_TEST_STRINGS.CHINESE_LATIN_MIXED,
            tokens: ["a", "##h", "\u535a", "\u63a8", "z", "##z"],
            ids: [101, 232, 247, 3166, 4657, 282, 283, 102],
            decoded: "[CLS] ah \u535a \u63a8 zz [SEP]",
        },
        MIXED_CASE_WITHOUT_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITHOUT_ACCENTS,
            tokens: ["He", "##LL", "##o", "!", "ho", "##w", "Are", "yo", "##U", "?"],
            ids: [101, 10869, 83346, 261, 106, 13685, 277, 14003, 14184, 211, 166, 102],
            decoded: "[CLS] HeLLo! how Are yoU? [SEP]",
        },
        MIXED_CASE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITH_ACCENTS,
            tokens: ["H", "##\u00e4", "##LL", "##o", "!", "ho", "##w", "Are", "yo", "##U", "?"],
            ids: [101, 184, 384, 83346, 261, 106, 13685, 277, 14003, 14184, 211, 166, 102],
            decoded: "[CLS] H\u00e4LLo! how Are yoU? [SEP]",
        },
    },
    "Xenova/kobert": {
        SIMPLE: {
            text: BASE_TEST_STRINGS.SIMPLE,
            tokens: ["[UNK]", "[UNK]", "[UNK]", "[UNK]", "?"],
            ids: [2, 0, 0, 0, 0, 258, 3],
            decoded: "[CLS] [UNK] [UNK] [UNK] [UNK]? [SEP]",
        },
        SIMPLE_WITH_PUNCTUATION: {
            text: BASE_TEST_STRINGS.SIMPLE_WITH_PUNCTUATION,
            tokens: ["[UNK]", "[UNK]", "'", "[UNK]", "[UNK]", "[UNK]"],
            ids: [2, 0, 0, 15, 0, 0, 0, 3],
            decoded: "[CLS] [UNK] [UNK]'[UNK] [UNK] [UNK] [SEP]",
        },
        TEXT_WITH_NUMBERS: {
            text: BASE_TEST_STRINGS.TEXT_WITH_NUMBERS,
            tokens: ["The", "[UNK]", "[UNK]", "[UNK]", "in", "[UNK]", "."],
            ids: [2, 355, 0, 0, 0, 409, 0, 54, 3],
            decoded: "[CLS] The [UNK] [UNK] [UNK] in [UNK]. [SEP]",
        },
        PUNCTUATION: {
            text: BASE_TEST_STRINGS.PUNCTUATION,
            tokens: ["A", "'", "[UNK]", "!", "!", "[UNK]", "?", "'", "d", "'", "'", "d", "[UNK]", ",", "[UNK]", "'", "t", "."],
            ids: [2, 264, 15, 0, 5, 5, 0, 258, 15, 388, 15, 15, 388, 0, 46, 0, 15, 442, 54, 3],
            decoded: "[CLS] A'[UNK]!! [UNK]?'d'' d [UNK], [UNK]'t. [SEP]",
        },
        PYTHON_CODE: {
            text: BASE_TEST_STRINGS.PYTHON_CODE,
            tokens: ["[UNK]", "[UNK]", "(", ")", ":", "[UNK]"],
            ids: [2, 0, 0, 18, 40, 249, 0, 3],
            decoded: "[CLS] [UNK] [UNK] ( ) : [UNK] [SEP]",
        },
        JAVASCRIPT_CODE: {
            text: BASE_TEST_STRINGS.JAVASCRIPT_CODE,
            tokens: ["[UNK]", "a", "=", "[UNK]", ".", "[UNK]", "(", ")", ";", "[UNK]", "(", ")", ";"],
            ids: [2, 0, 367, 254, 0, 54, 0, 18, 40, 252, 0, 18, 40, 252, 3],
            decoded: "[CLS] [UNK] a = [UNK]. [UNK] ( ) ; [UNK] ( ) ; [SEP]",
        },
        NEWLINES: {
            text: BASE_TEST_STRINGS.NEWLINES,
            tokens: ["[UNK]", "is", "a", "[UNK]", "."],
            ids: [2, 0, 412, 367, 0, 54, 3],
            decoded: "[CLS] [UNK] is a [UNK]. [SEP]",
        },
        BASIC: {
            text: BASE_TEST_STRINGS.BASIC,
            tokens: ["[UNK]", ",", "[UNK]"],
            ids: [2, 0, 46, 0, 3],
            decoded: "[CLS] [UNK], [UNK] [SEP]",
        },
        CHINESE_ONLY: {
            text: BASE_TEST_STRINGS.CHINESE_ONLY,
            tokens: ["\u751f", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]"],
            ids: [2, 5298, 0, 0, 0, 0, 0, 3],
            decoded: "[CLS] \u751f [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]",
        },
        CURRENCY: {
            text: BASE_TEST_STRINGS.CURRENCY,
            tokens: ["[UNK]", "$", "1", "[UNK]", "#", "3", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]"],
            ids: [2, 0, 10, 93, 0, 9, 142, 0, 0, 0, 0, 0, 0, 0, 3],
            decoded: "[CLS] [UNK] $ 1 [UNK] # 3 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]",
        },
        CURRENCY_WITH_DECIMALS: {
            text: BASE_TEST_STRINGS.CURRENCY_WITH_DECIMALS,
            tokens: ["I", "[UNK]", "an", "[UNK]", "[UNK]", "$", "1", ".", "00", "at", "[UNK]", "[UNK]", "."],
            ids: [2, 296, 0, 374, 0, 0, 10, 93, 54, 79, 377, 0, 0, 54, 3],
            decoded: "[CLS] I [UNK] an [UNK] [UNK] $ 1. 00 at [UNK] [UNK]. [SEP]",
        },
        TEXT_WITH_ESCAPE_CHARACTERS_2: {
            text: BASE_TEST_STRINGS.TEXT_WITH_ESCAPE_CHARACTERS_2,
            tokens: ["[UNK]", "[UNK]", "[UNK]", "[UNK]"],
            ids: [2, 0, 0, 0, 0, 3],
            decoded: "[CLS] [UNK] [UNK] [UNK] [UNK] [SEP]",
        },
        TILDE_NORMALIZATION: {
            text: BASE_TEST_STRINGS.TILDE_NORMALIZATION,
            tokens: ["[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]"],
            ids: [2, 0, 0, 0, 0, 0, 3],
            decoded: "[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]",
        },
        SPIECE_UNDERSCORE: {
            text: BASE_TEST_STRINGS.SPIECE_UNDERSCORE,
            tokens: ["[UNK]", "[UNK]", "[UNK]", "[UNK]", "\u2581", "."],
            ids: [2, 0, 0, 0, 0, 517, 54, 3],
            decoded: "[CLS] [UNK] [UNK] [UNK] [UNK] \u2581. [SEP]",
        },
        CHINESE_LATIN_MIXED: {
            text: BERT_TEST_STRINGS.CHINESE_LATIN_MIXED,
            tokens: ["[UNK]", "[UNK]", "[UNK]", "[UNK]"],
            ids: [2, 0, 0, 0, 0, 3],
            decoded: "[CLS] [UNK] [UNK] [UNK] [UNK] [SEP]",
        },
        MIXED_CASE_WITHOUT_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITHOUT_ACCENTS,
            tokens: ["[UNK]", "!", "[UNK]", "[UNK]", "[UNK]", "?"],
            ids: [2, 0, 5, 0, 0, 0, 258, 3],
            decoded: "[CLS] [UNK]! [UNK] [UNK] [UNK]? [SEP]",
        },
        MIXED_CASE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITH_ACCENTS,
            tokens: ["[UNK]", "!", "[UNK]", "[UNK]", "[UNK]", "?"],
            ids: [2, 0, 5, 0, 0, 0, 258, 3],
            decoded: "[CLS] [UNK]! [UNK] [UNK] [UNK]? [SEP]",
        },
    },
    "Xenova/scibert_scivocab_uncased": {
        JAVASCRIPT_CODE: {
            text: BASE_TEST_STRINGS.JAVASCRIPT_CODE,
            tokens: ["let", "a", "=", "obj", ".", "to", "##string", "(", ")", ";", "to", "##string", "(", ")", ";"],
            ids: [102, 1293, 106, 275, 2324, 205, 147, 20301, 145, 546, 1814, 147, 20301, 145, 546, 1814, 103],
            decoded: "[CLS] let a = obj. tostring ( ) ; tostring ( ) ; [SEP]",
        },
        DOUBLE_SPACE: {
            text: BASE_TEST_STRINGS.DOUBLE_SPACE,
            tokens: ["hi", "hell", "##o"],
            ids: [102, 5305, 29423, 30112, 103],
            decoded: "[CLS] hi hello [SEP]",
        },
        CURRENCY: {
            text: BASE_TEST_STRINGS.CURRENCY,
            tokens: ["test", "$", "1", "r", "##2", "#", "3", "\u20ac", "##4", "\u00a3", "##5", "\u00a5", "##6", "[UNK]", "[UNK]", "[UNK]", "test"],
            ids: [102, 856, 3250, 158, 182, 30132, 3000, 239, 20801, 30140, 11221, 30139, 20704, 30142, 101, 101, 101, 856, 103],
            decoded: "[CLS] test $ 1 r2 # 3 \u20ac4 \u00a35 \u00a56 [UNK] [UNK] [UNK] test [SEP]",
        },
        CHINESE_LATIN_MIXED: {
            text: BERT_TEST_STRINGS.CHINESE_LATIN_MIXED,
            tokens: ["ah", "[UNK]", "[UNK]", "zz"],
            ids: [102, 7839, 101, 101, 23591, 103],
            decoded: "[CLS] ah [UNK] [UNK] zz [SEP]",
        },
        SIMPLE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.SIMPLE_WITH_ACCENTS,
            tokens: ["hell", "##o"],
            ids: [102, 29423, 30112, 103],
            decoded: "[CLS] hello [SEP]",
        },
        MIXED_CASE_WITHOUT_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITHOUT_ACCENTS,
            tokens: ["hell", "##o", "!", "how", "are", "you", "?"],
            ids: [102, 29423, 30112, 3190, 539, 220, 3034, 3912, 103],
            decoded: "[CLS] hello! how are you? [SEP]",
        },
    },
    "Xenova/LaBSE": {
        JAVASCRIPT_CODE: {
            text: BASE_TEST_STRINGS.JAVASCRIPT_CODE,
            tokens: ["let", "a", "=", "obj", ".", "to", "##String", "(", ")", ";", "to", "##String", "(", ")", ";"],
            ids: [101, 17214, 170, 134, 228877, 119, 14986, 368304, 113, 114, 132, 14986, 368304, 113, 114, 132, 102],
            decoded: "[CLS] let a = obj. toString ( ) ; toString ( ) ; [SEP]",
        },
        CURRENCY: {
            text: BASE_TEST_STRINGS.CURRENCY,
            tokens: ["test", "$", "1", "R2", "#", "3", "\u20ac", "##4", "\u00a35", "\u00a5", "##6", "\u20a3", "##7", "\u20b9", "##8", "\u20b1", "##9", "test"],
            ids: [101, 17678, 109, 122, 51222, 108, 124, 3030, 16006, 279082, 205, 16151, 3023, 16187, 3037, 16175, 3033, 16236, 17678, 102],
            decoded: "[CLS] test $ 1 R2 # 3 \u20ac4 \u00a35 \u00a56 \u20a37 \u20b98 \u20b19 test [SEP]",
        },
        SPIECE_UNDERSCORE: {
            text: BASE_TEST_STRINGS.SPIECE_UNDERSCORE,
            tokens: ["\u2581", "##This", "\u2581", "##is", "\u2581", "##a", "\u2581", "##test", "\u2581", "."],
            ids: [101, 3283, 342068, 3283, 15319, 3283, 14983, 3283, 50149, 3283, 119, 102],
            decoded: "[CLS] \u2581This \u2581is \u2581a \u2581test \u2581. [SEP]",
        },
        POPULAR_EMOJIS: {
            text: BASE_TEST_STRINGS.POPULAR_EMOJIS,
            tokens: ["\ud83d\ude02", "\ud83d\udc4d", "\ud83e\udd23", "\ud83d\ude0d", "\ud83d\ude2d", "\ud83c\udf89", "\ud83d\ude4f", "\ud83d\ude0a", "\ud83d\udd25", "\ud83d\ude01", "\ud83d\ude05", "\ud83e\udd17", "\ud83d\ude06", "\ud83d\udc4f", "\u2764\ufe0f", "\ud83d\udc9c", "\ud83d\udc9a", "\ud83d\udc97", "\ud83d\udc99", "\ud83d\udda4", "\ud83d\ude0e", "\ud83d\udc4c", "\ud83e\udd73", "\ud83d\udcaa", "\u2728", "\ud83d\udc49", "\ud83d\udc40", "\ud83d\udcaf", "\ud83c\udf88", "\ud83d\ude48", "\ud83d\ude4c", "\ud83d\udc80", "\ud83d\udc47", "\ud83d\udc4b", "\u2705", "\ud83c\udf81", "\ud83c\udf1e", "\ud83c\udf38", "\ud83d\udcb0"],
            ids: [101, 14820, 14617, 14933, 14831, 14863, 14496, 14893, 14828, 14775, 14819, 14823, 14926, 14824, 14619, 91822, 14687, 14685, 14682, 14684, 14810, 14832, 14616, 14956, 14701, 3496, 14613, 14606, 14706, 14495, 14887, 14891, 14660, 14611, 14615, 3465, 14488, 14416, 14430, 14707, 102],
            decoded: "[CLS] \ud83d\ude02 \ud83d\udc4d \ud83e\udd23 \ud83d\ude0d \ud83d\ude2d \ud83c\udf89 \ud83d\ude4f \ud83d\ude0a \ud83d\udd25 \ud83d\ude01 \ud83d\ude05 \ud83e\udd17 \ud83d\ude06 \ud83d\udc4f \u2764\ufe0f \ud83d\udc9c \ud83d\udc9a \ud83d\udc97 \ud83d\udc99 \ud83d\udda4 \ud83d\ude0e \ud83d\udc4c \ud83e\udd73 \ud83d\udcaa \u2728 \ud83d\udc49 \ud83d\udc40 \ud83d\udcaf \ud83c\udf88 \ud83d\ude48 \ud83d\ude4c \ud83d\udc80 \ud83d\udc47 \ud83d\udc4b \u2705 \ud83c\udf81 \ud83c\udf1e \ud83c\udf38 \ud83d\udcb0 [SEP]",
        },
        MULTIBYTE_EMOJIS: {
            text: BASE_TEST_STRINGS.MULTIBYTE_EMOJIS,
            tokens: ["\u2728", "\ud83e\udd17", "\ud83d\udc41\ufe0f", "\ud83d\udc71", "##\ud83c\udffb", "[UNK]", "[UNK]", "\ud83d\udc68", "##\ud83c\udffb", "##\ud83c\udf3e", "[UNK]", "\ud83d\udc69", "##\u2764", "##\ud83d\udc8b", "##\ud83d\udc68", "\ud83d\udc69", "##\ud83d\udc69", "##\ud83d\udc67", "##\ud83d\udc66", "[UNK]", "\ud83c\udff4", "\ud83d\udc68", "##\ud83c\udffb", "##\u2764", "##\ufe0f", "##\ud83d\udc8b", "##\ud83d\udc68", "##\ud83c\udffc"],
            ids: [101, 3496, 14926, 350545, 14648, 130826, 100, 100, 14639, 130826, 498832, 100, 14640, 488649, 499065, 499034, 14640, 499035, 499033, 499032, 100, 14555, 14639, 130826, 488649, 44450, 499065, 499034, 421916, 102],
            decoded: "[CLS] \u2728 \ud83e\udd17 \ud83d\udc41\ufe0f \ud83d\udc71\ud83c\udffb [UNK] [UNK] \ud83d\udc68\ud83c\udffb\ud83c\udf3e [UNK] \ud83d\udc69\u2764\ud83d\udc8b\ud83d\udc68 \ud83d\udc69\ud83d\udc69\ud83d\udc67\ud83d\udc66 [UNK] \ud83c\udff4 \ud83d\udc68\ud83c\udffb\u2764\ufe0f\ud83d\udc8b\ud83d\udc68\ud83c\udffc [SEP]",
        },
        CHINESE_LATIN_MIXED: {
            text: BERT_TEST_STRINGS.CHINESE_LATIN_MIXED,
            tokens: ["ah", "\u535a", "\u63a8", "zz"],
            ids: [101, 15524, 4573, 6405, 441764, 102],
            decoded: "[CLS] ah \u535a \u63a8 zz [SEP]",
        },
        SIMPLE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.SIMPLE_WITH_ACCENTS,
            tokens: ["H\u00e9", "##llo"],
            ids: [101, 220855, 23025, 102],
            decoded: "[CLS] H\u00e9llo [SEP]",
        },
    },
    "Xenova/herbert-large-cased": {
        SIMPLE: {
            text: BASE_TEST_STRINGS.SIMPLE,
            tokens: ["Ho", "w</w>", "are</w>", "you</w>", "do", "ing</w>", "?</w>"],
            ids: [0, 5213, 1019, 25720, 20254, 2065, 5129, 1550, 2],
            decoded: "<s>How are you doing? </s>",
        },
        SIMPLE_WITH_PUNCTUATION: {
            text: BASE_TEST_STRINGS.SIMPLE_WITH_PUNCTUATION,
            tokens: ["You</w>", "sho", "uld</w>", "'</w>", "ve</w>", "d", "one</w>", "this</w>"],
            ids: [0, 32795, 14924, 48273, 1571, 6647, 72, 2290, 48846, 2],
            decoded: "<s>You should've done this </s>",
        },
        TEXT_WITH_NUMBERS: {
            text: BASE_TEST_STRINGS.TEXT_WITH_NUMBERS,
            tokens: ["The</w>", "co", "mpany</w>", "was</w>", "fo", "un", "de", "d</w>", "in</w>", "20", "16</w>", ".</w>"],
            ids: [0, 7117, 2406, 41449, 9873, 3435, 2195, 2101, 1038, 2651, 5646, 2555, 1899, 2],
            decoded: "<s>The company was founded in 2016. </s>",
        },
        PUNCTUATION: {
            text: BASE_TEST_STRINGS.PUNCTUATION,
            tokens: ["A</w>", "'</w>", "ll</w>", "!</w>", "!</w>", "to</w>", "?</w>", "'</w>", "d</w>", "'</w>", "'</w>", "d</w>", "of</w>", ",</w>", "can</w>", "'</w>", "t</w>", ".</w>"],
            ids: [0, 1012, 1571, 9396, 1725, 1725, 2063, 1550, 1571, 1038, 1571, 1571, 1038, 6595, 1947, 26794, 1571, 1026, 1899, 2],
            decoded: "<s>A'll!! to?'d'' d of, can't. </s>",
        },
        PYTHON_CODE: {
            text: BASE_TEST_STRINGS.PYTHON_CODE,
            tokens: ["de", "f</w>", "main</w>", "(</w>", ")</w>", ":</w>", "pa", "ss</w>"],
            ids: [0, 2101, 1050, 41851, 1341, 1940, 1335, 2083, 5357, 2],
            decoded: "<s>def main ( ) : pass </s>",
        },
        JAVASCRIPT_CODE: {
            text: BASE_TEST_STRINGS.JAVASCRIPT_CODE,
            tokens: ["let</w>", "a</w>", "=</w>", "ob", "j</w>", ".</w>", "to", "S", "tr", "ing</w>", "(</w>", ")</w>", ";</w>", "to", "S", "tr", "ing</w>", "(</w>", ")</w>", ";</w>"],
            ids: [0, 11324, 1011, 1789, 2033, 1013, 1899, 2146, 55, 2518, 5129, 1341, 1940, 1195, 2146, 55, 2518, 5129, 1341, 1940, 1195, 2],
            decoded: "<s>let a = obj. toString ( ) ; toString ( ) ; </s>",
        },
        NEWLINES: {
            text: BASE_TEST_STRINGS.NEWLINES,
            tokens: ["T", "his</w>", "is</w>", "a</w>", "test</w>", ".</w>"],
            ids: [0, 56, 22855, 6869, 1011, 14825, 1899, 2],
            decoded: "<s>This is a test. </s>",
        },
        BASIC: {
            text: BASE_TEST_STRINGS.BASIC,
            tokens: ["UN", "wan", "t", "\u00e9", "d</w>", ",</w>", "run", "ning</w>"],
            ids: [0, 23029, 2688, 88, 163, 1038, 1947, 4980, 17843, 2],
            decoded: "<s>UNwant\u00e9d, running </s>",
        },
        CONTROL_TOKENS: {
            text: BASE_TEST_STRINGS.CONTROL_TOKENS,
            tokens: ["123</w>"],
            ids: [0, 19049, 2],
            decoded: "<s>123 </s>",
        },
        HELLO_WORLD_TITLECASE: {
            text: BASE_TEST_STRINGS.HELLO_WORLD_TITLECASE,
            tokens: ["Hel", "lo</w>", "World</w>"],
            ids: [0, 12156, 6170, 21207, 2],
            decoded: "<s>Hello World </s>",
        },
        HELLO_WORLD_LOWERCASE: {
            text: BASE_TEST_STRINGS.HELLO_WORLD_LOWERCASE,
            tokens: ["hel", "lo</w>", "world</w>"],
            ids: [0, 11526, 6170, 38188, 2],
            decoded: "<s>hello world </s>",
        },
        CHINESE_ONLY: {
            text: BASE_TEST_STRINGS.CHINESE_ONLY,
            tokens: ["<unk>", "<unk>", "<unk>", "<unk>", "<unk>", "\u662f</w>"],
            ids: [0, 3, 3, 3, 3, 3, 1776, 2],
            decoded: "<s><unk><unk><unk><unk><unk>\u662f </s>",
        },
        LEADING_SPACE: {
            text: BASE_TEST_STRINGS.LEADING_SPACE,
            tokens: ["le", "ad", "ing</w>", "space</w>"],
            ids: [0, 2018, 2035, 5129, 46489, 2],
            decoded: "<s>leading space </s>",
        },
        TRAILING_SPACE: {
            text: BASE_TEST_STRINGS.TRAILING_SPACE,
            tokens: ["tra", "i", "ling</w>", "space</w>"],
            ids: [0, 2201, 77, 16342, 46489, 2],
            decoded: "<s>trailing space </s>",
        },
        DOUBLE_SPACE: {
            text: BASE_TEST_STRINGS.DOUBLE_SPACE,
            tokens: ["H", "i</w>", "Hel", "lo</w>"],
            ids: [0, 44, 1009, 12156, 6170, 2],
            decoded: "<s>Hi Hello </s>",
        },
        CURRENCY: {
            text: BASE_TEST_STRINGS.CURRENCY,
            tokens: ["test</w>", "$</w>", "1</w>", "R", "2</w>", "#</w>", "3</w>", "\u20ac", "4</w>", "\u00a3", "5</w>", "<unk>", "6</w>", "<unk>", "7</w>", "<unk>", "8</w>", "<unk>", "9</w>", "test</w>"],
            ids: [0, 14825, 1927, 1029, 54, 1025, 1393, 1034, 706, 1018, 100, 1008, 3, 1036, 3, 1030, 3, 1064, 3, 1017, 14825, 2],
            decoded: "<s>test $ 1 R2 # 3 \u20ac4 \u00a35 <unk>6 <unk>7 <unk>8 <unk>9 test </s>",
        },
        CURRENCY_WITH_DECIMALS: {
            text: BASE_TEST_STRINGS.CURRENCY_WITH_DECIMALS,
            tokens: ["I</w>", "bou", "ght</w>", "an</w>", "ap", "ple</w>", "for</w>", "$</w>", "1</w>", ".</w>", "00</w>", "at</w>", "the</w>", "st", "ore</w>", ".</w>"],
            ids: [0, 1056, 13016, 15272, 2879, 10309, 20861, 15181, 1927, 1029, 1899, 2291, 4772, 6854, 1989, 24005, 1899, 2],
            decoded: "<s>I bought an apple for $ 1. 00 at the store. </s>",
        },
        ELLIPSIS: {
            text: BASE_TEST_STRINGS.ELLIPSIS,
            tokens: ["you</w>", "\u2026</w>"],
            ids: [0, 20254, 1826, 2],
            decoded: "<s>you \u2026 </s>",
        },
        TEXT_WITH_ESCAPE_CHARACTERS: {
            text: BASE_TEST_STRINGS.TEXT_WITH_ESCAPE_CHARACTERS,
            tokens: ["you</w>", "\u2026</w>"],
            ids: [0, 20254, 1826, 2],
            decoded: "<s>you \u2026 </s>",
        },
        TEXT_WITH_ESCAPE_CHARACTERS_2: {
            text: BASE_TEST_STRINGS.TEXT_WITH_ESCAPE_CHARACTERS_2,
            tokens: ["you</w>", "\u2026</w>", "you</w>", "\u2026</w>"],
            ids: [0, 20254, 1826, 20254, 1826, 2],
            decoded: "<s>you \u2026 you \u2026 </s>",
        },
        TILDE_NORMALIZATION: {
            text: BASE_TEST_STRINGS.TILDE_NORMALIZATION,
            tokens: ["we", "ir", "d</w>", "<unk>", "e", "dge</w>", "<unk>", "ca", "se</w>"],
            ids: [0, 2149, 17435, 1038, 3, 73, 25801, 3, 3833, 4417, 2],
            decoded: "<s>weird <unk>edge <unk>case </s>",
        },
        SPIECE_UNDERSCORE: {
            text: BASE_TEST_STRINGS.SPIECE_UNDERSCORE,
            tokens: ["<unk>", "T", "his</w>", "<unk>", "is</w>", "<unk>", "a</w>", "<unk>", "test</w>", "<unk>", ".</w>"],
            ids: [0, 3, 56, 22855, 3, 6869, 3, 1011, 3, 14825, 3, 1899, 2],
            decoded: "<s><unk>This <unk>is <unk>a <unk>test <unk>. </s>",
        },
        CHINESE_LATIN_MIXED: {
            text: BERT_TEST_STRINGS.CHINESE_LATIN_MIXED,
            tokens: ["a", "h</w>", "<unk>", "<unk>", "zz</w>"],
            ids: [0, 69, 1021, 3, 3, 49185, 2],
            decoded: "<s>ah <unk><unk>zz </s>",
        },
        SIMPLE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.SIMPLE_WITH_ACCENTS,
            tokens: ["H", "\u00e9", "l", "lo</w>"],
            ids: [0, 44, 163, 80, 6170, 2],
            decoded: "<s>H\u00e9llo </s>",
        },
        MIXED_CASE_WITHOUT_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITHOUT_ACCENTS,
            tokens: ["He", "L", "L", "o</w>", "!</w>", "ho", "w</w>", "Ar", "e</w>", "yo", "U</w>", "?</w>"],
            ids: [0, 4596, 48, 48, 1007, 1725, 3145, 1019, 2921, 1015, 13908, 1041, 1550, 2],
            decoded: "<s>HeLLo! how Are yoU? </s>",
        },
        MIXED_CASE_WITH_ACCENTS: {
            text: BERT_TEST_STRINGS.MIXED_CASE_WITH_ACCENTS,
            tokens: ["H", "\u00e4", "L", "L", "o</w>", "!</w>", "ho", "w</w>", "Ar", "e</w>", "yo", "U</w>", "?</w>"],
            ids: [0, 44, 158, 48, 48, 1007, 1725, 3145, 1019, 2921, 1015, 13908, 1041, 1550, 2],
            decoded: "<s>H\u00e4LLo! how Are yoU? </s>",
        },
    },
    "Xenova/ernie-gram-zh": {
        CURRENCY: {
            text: BASE_TEST_STRINGS.CURRENCY,
            tokens: ["test", "$", "1", "r2", "#", "3", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "[UNK]", "test"],
            ids: [1, 6943, 18005, 208, 6847, 9474, 284, 18017, 18017, 18017, 18017, 18017, 18017, 6943, 2],
            decoded: "[CLS] test $ 1 r2 # 3 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] test [SEP]",
        },
    },
};
